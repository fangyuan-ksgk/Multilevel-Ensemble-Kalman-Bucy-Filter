{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dX_t = A X_t dt + R_1 dW_t$$\n",
    "$$dY_t = H X_t dt + R_2 dV_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model settings: (Conditions that the model needs to satsify)\n",
    "1. Signal of dimension 'dim', and the observtations are in dimension 'dim_o'\n",
    "2. $x_0|y_0 \\sim N(m_0,C_0)$\n",
    "3. $R_1, R_2$ is symmetric\n",
    "4. $R_1$ is commutable with $A$\n",
    "5. $(A+A')$ is invertible\n",
    "6. For convenience purpose the target function is $\\varphi(x)=x^{1}$, where $x=(x^{1},x^{2},...,x^{dim})$\n",
    "\n",
    "In this code, I will include the $A,R_1,R_2,H,m_0,C_0$ as parameter as well, so that it's easily scalable wrt any dimension dim.\n",
    "Note that dim needs to be bigger than 1, as the matrix multiplication sign '@' in python doesn't support '*' in one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import progressbar\n",
    "from scipy import linalg as la\n",
    "from scipy.sparse import identity\n",
    "from scipy.sparse import rand\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import triu\n",
    "\n",
    "def gen_model(dim):\n",
    "    ## dim is dimension value\n",
    "    dim_o=dim\n",
    "    A1 = -rand(dim,dim,density=0.75).toarray()/5\n",
    "    A2 = triu(A1, k=1).toarray()/(10)\n",
    "    A = diags(np.random.normal(-0.5,0,dim),0).toarray()/50 + A2 - A2.T\n",
    "    ## we denote R1^{1/2},R2^{1/2} as R1,R2 repectively for convenience\n",
    "    ## Non-Identity covariance matrix\n",
    "    R1=(identity(dim).toarray() + np.tri(dim,dim,1) - np.tri(dim,dim,-2))/2\n",
    "    R2=(identity(dim_o).toarray() + np.tri(dim_o,dim_o,1) - np.tri(dim_o,dim_o,-2))/2\n",
    "\n",
    "    H=rand(dim_o,dim,density=0.75).toarray()/20\n",
    "    m0=np.zeros(dim)+6\n",
    "    C0=identity(dim).toarray()\n",
    "    ## Collection of input \n",
    "    collection_input = [dim,dim_o,A,R1,R2,H,m0,C0]\n",
    "    return collection_input\n",
    "\n",
    "def gen_data(T,l,collection_input):\n",
    "    \n",
    "    [dim,dim_o,A,R1,R2,H,m0,C0]=collection_input\n",
    "    J=T*(2**l)\n",
    "    I=identity(dim).toarray()\n",
    "    tau=2**(-l)\n",
    "    L=la.expm(A*tau)\n",
    "    ## We are going to need W to be symmetric! \n",
    "    W=(R1@R1)@(la.inv(A+A.T)@(L@(L.T)-I))\n",
    "    C=tau*H\n",
    "    V=(R2@R2)*tau\n",
    "\n",
    "    v=np.zeros((J+1,dim,1))\n",
    "    z=np.zeros((J+1,dim_o,1))\n",
    "    v[0]=np.random.multivariate_normal(m0,C0,(1)).T\n",
    "    z[0]=np.zeros((dim_o,1))\n",
    "\n",
    "\n",
    "    for j in range(J):\n",
    "        ## truth\n",
    "        v[j+1] = L@v[j] + np.random.multivariate_normal(np.zeros(dim),W,(1)).T\n",
    "        ## observation\n",
    "        z[j+1] = z[j] + C@v[j+1] + np.random.multivariate_normal(np.zeros(dim_o),V,(1)).T\n",
    "        \n",
    "    return([z,v])\n",
    "\n",
    "def cut(T,lmax,l,v):\n",
    "    ind = np.arange(T*2**l+1)\n",
    "    rtau = 2**(lmax-l)\n",
    "    w = v[ind*rtau]\n",
    "    return(w)\n",
    "\n",
    "def EnKBF(T,l,lmax,z,N,collection_input):\n",
    "    \n",
    "    [dim,dim_o,A,R1,R2,H,m0,C0]=collection_input\n",
    "    J=T*(2**l)\n",
    "    I=identity(dim).toarray()\n",
    "    I_o=identity(dim_o).toarray()\n",
    "    dt=2**(-l)\n",
    "    \n",
    "    m=np.zeros((J+1,dim,1))\n",
    "    c=np.zeros((J+1,dim,dim))\n",
    "    z=cut(T,lmax,l,z)\n",
    "    \n",
    "    ## This gives a dim*N matrix\n",
    "    x = np.random.multivariate_normal(m0,C0,N).T\n",
    "    ## A dim*1 vector\n",
    "    m[0]=(np.mean(x, axis=1)).reshape(dim,1)\n",
    "    ## dim*dim matrix\n",
    "    c[0]=((x-m[0])@((x-m[0]).T)) /(N-1)\n",
    "    for j in range(J):\n",
    "        dw = np.random.multivariate_normal(np.zeros(dim),dt*I,N).T\n",
    "        dv = np.random.multivariate_normal(np.zeros(dim_o),dt*I_o,N).T\n",
    "        ## A@x:dim*N R1@dw:dim*N c[j]@(H.T):dim*dim_o z[j+1]-z[j]:dim_o*1 H@x*dt:dim_o*N R2*dv:dim_o*N \n",
    "        ## x-m[j]:dim*N c[j]:dim*dim\n",
    "        \n",
    "        step1=(((x-m[j]).T)@(H.T))\n",
    "        step2=step1@(la.inv(R2)@la.inv(R2))\n",
    "        step3=step2@( (z[j+1]-z[j]) - (H@x*dt + R2@dv) )\n",
    "        step4=(x-m[j])@step3 /(N-1)\n",
    "        \n",
    "        x = x + A@x*dt + R1@dw + step4\n",
    "        m[j+1] = (np.mean(x, axis=1)).reshape(dim,1)\n",
    "\n",
    "    return([m,c])\n",
    "\n",
    "def CEnKBF(T,l,lmax,z,N,collection_input):\n",
    "    \n",
    "    [dim,dim_o,A,R1,R2,H,m0,C0]=collection_input\n",
    "    J=T*(2**(l-1))\n",
    "    I=identity(dim).toarray()\n",
    "    I1=identity(dim_o).toarray()\n",
    "    dt=2**(-l)\n",
    "    dt1=2**(-l+1)\n",
    "    \n",
    "    m=np.zeros((J*2+1,dim,1))\n",
    "    m1=np.zeros((J+1,dim,1))\n",
    "    c=np.zeros((J*2+1,dim,dim))\n",
    "    c1=np.zeros((J+1,dim,dim))\n",
    "    z1=cut(T,lmax,l-1,z)\n",
    "    z=cut(T,lmax,l,z)\n",
    "    \n",
    "    ## This gives a dim*N matrix\n",
    "    x = np.random.multivariate_normal(m0,C0,N).T\n",
    "    x1 = x\n",
    "    ## A dim*1 vector\n",
    "    m[0]=(np.mean(x, axis=1)).reshape(dim,1)\n",
    "    m1[0]=m[0]\n",
    "    ## dim*dim matrix\n",
    "    c[0]=((x-m[0])@((x-m[0]).T)) /(N-1)\n",
    "    c1[0]=c[0]\n",
    "    \n",
    "    dw=np.zeros((2,dim,N))\n",
    "    dv=np.zeros((2,dim_o,N))\n",
    "    for j in range(J):\n",
    "        for s in range(2):\n",
    "            dw[s] = np.random.multivariate_normal(np.zeros(dim),dt*I,N).T\n",
    "            dv[s] = np.random.multivariate_normal(np.zeros(dim_o),dt*I1,N).T\n",
    "            ## A@x:dim*N R1@dw:dim*N c[j]@(H.T):dim*dim_o z[j+1]-z[j]:dim_o*1 H@x*dt:dim_o*N R2*dv:dim_o*N \n",
    "            ## x-m[j]:dim*N c[j]:dim*dim\n",
    "\n",
    "            step1=(((x-m[2*j+s]).T)@(H.T))\n",
    "            step2=step1@(la.inv(R2)@la.inv(R2))\n",
    "            step3=step2@( (z[2*j+s+1]-z[2*j+s]) - (H@x*dt + R2@dv[s]) )\n",
    "            step4=(x-m[2*j+s])@step3 /(N-1)\n",
    "\n",
    "            x = x + A@x*dt + R1@dw[s] + step4\n",
    "            m[2*j+s+1] = (np.mean(x, axis=1)).reshape(dim,1)\n",
    "        \n",
    "        step1=(((x1-m1[j]).T)@(H.T))\n",
    "        step2=step1@(la.inv(R2)@la.inv(R2))\n",
    "        step3=step2@( (z1[j+1]-z1[j]) - (H@x1*dt1 + R2@(dv[0]+dv[1])) )\n",
    "        step4=(x1-m1[j])@step3 /(N-1)\n",
    "        \n",
    "        x1 = x1 + A@x1*dt1 + R1@(dw[0]+dw[1]) + step4\n",
    "        m1[j+1] = (np.mean(x1, axis=1)).reshape(dim,1)\n",
    "        \n",
    "    return([m,m1])\n",
    "\n",
    "def DEnKBF(T,l,lmax,z,N,collection_input):\n",
    "    \n",
    "    [dim,dim_o,A,R1,R2,H,m0,C0]=collection_input\n",
    "    J=T*(2**l)\n",
    "    I=identity(dim).toarray()\n",
    "    I_o=identity(dim_o).toarray()\n",
    "    dt=2**(-l)\n",
    "    \n",
    "    m=np.zeros((J+1,dim,1))\n",
    "    c=np.zeros((J+1,dim,dim))\n",
    "    z=cut(T,lmax,l,z)\n",
    "    \n",
    "    ## This gives a dim*N matrix\n",
    "    x = np.random.multivariate_normal(m0,C0,N).T\n",
    "    ## A dim*1 vector\n",
    "    m[0]=(np.mean(x, axis=1)).reshape(dim,1)\n",
    "    ## dim*dim matrix\n",
    "    c[0]=((x-m[0])@((x-m[0]).T)) /(N-1)\n",
    "    for j in range(J):\n",
    "        dw = np.random.multivariate_normal(np.zeros(dim),dt*I,N).T\n",
    "        dv = np.random.multivariate_normal(np.zeros(dim_o),dt*I_o,N).T\n",
    "        ## A@x:dim*N R1@dw:dim*N c[j]@(H.T):dim*dim_o z[j+1]-z[j]:dim_o*1 H@x*dt:dim_o*N R2*dv:dim_o*N \n",
    "        ## x-m[j]:dim*N c[j]:dim*dim\n",
    "        \n",
    "        step1=(((x-m[j]).T)@(H.T))\n",
    "        step2=step1@(la.inv(R2)@la.inv(R2))\n",
    "        # Only the \"innovation\" term here changes to the \"deterministic\" version\n",
    "        step3=step2@( (z[j+1]-z[j]) - (H@(x+m[j])*dt)/2 )\n",
    "        step4=(x-m[j])@step3 /(N-1)\n",
    "        \n",
    "        x = x + A@x*dt + R1@dw + step4\n",
    "        m[j+1] = (np.mean(x, axis=1)).reshape(dim,1)\n",
    "\n",
    "    return([m,c])\n",
    "\n",
    "def DCEnKBF(T,l,lmax,z,N,collection_input):\n",
    "    \n",
    "    [dim,dim_o,A,R1,R2,H,m0,C0]=collection_input\n",
    "    J=T*(2**(l-1))\n",
    "    I=identity(dim).toarray()\n",
    "    I1=identity(dim_o).toarray()\n",
    "    dt=2**(-l)\n",
    "    dt1=2**(-l+1)\n",
    "    \n",
    "    m=np.zeros((J*2+1,dim,1))\n",
    "    m1=np.zeros((J+1,dim,1))\n",
    "    c=np.zeros((J*2+1,dim,dim))\n",
    "    c1=np.zeros((J+1,dim,dim))\n",
    "    z1=cut(T,lmax,l-1,z)\n",
    "    z=cut(T,lmax,l,z)\n",
    "    \n",
    "    ## This gives a dim*N matrix\n",
    "    x = np.random.multivariate_normal(m0,C0,N).T\n",
    "    x1 = x\n",
    "    ## A dim*1 vector\n",
    "    m[0]=(np.mean(x, axis=1)).reshape(dim,1)\n",
    "    m1[0]=m[0]\n",
    "    ## dim*dim matrix\n",
    "    c[0]=((x-m[0])@((x-m[0]).T)) /(N-1)\n",
    "    c1[0]=c[0]\n",
    "    \n",
    "    dw=np.zeros((2,dim,N))\n",
    "    dv=np.zeros((2,dim_o,N))\n",
    "    for j in range(J):\n",
    "        for s in range(2):\n",
    "            dw[s] = np.random.multivariate_normal(np.zeros(dim),dt*I,N).T\n",
    "            dv[s] = np.random.multivariate_normal(np.zeros(dim_o),dt*I1,N).T\n",
    "            ## A@x:dim*N R1@dw:dim*N c[j]@(H.T):dim*dim_o z[j+1]-z[j]:dim_o*1 H@x*dt:dim_o*N R2*dv:dim_o*N \n",
    "            ## x-m[j]:dim*N c[j]:dim*dim\n",
    "\n",
    "            step1=(((x-m[2*j+s]).T)@(H.T))\n",
    "            step2=step1@(la.inv(R2)@la.inv(R2))\n",
    "            step3=step2@( (z[2*j+s+1]-z[2*j+s]) - (H@(x+m[2*j+s])*dt)/2 )\n",
    "            step4=(x-m[2*j+s])@step3 /(N-1)\n",
    "\n",
    "            x = x + A@x*dt + R1@dw[s] + step4\n",
    "            m[2*j+s+1] = (np.mean(x, axis=1)).reshape(dim,1)\n",
    "        \n",
    "        step1=(((x1-m1[j]).T)@(H.T))\n",
    "        step2=step1@(la.inv(R2)@la.inv(R2))\n",
    "        \n",
    "        step3=step2@( (z1[j+1]-z1[j]) - (H@(x1+m1[j])*dt1)/2 )\n",
    "        step4=(x1-m1[j])@step3 /(N-1)\n",
    "        \n",
    "        x1 = x1 + A@x1*dt1 + R1@(dw[0]+dw[1]) + step4\n",
    "        m1[j+1] = (np.mean(x1, axis=1)).reshape(dim,1)\n",
    "        \n",
    "    return([m,m1])\n",
    "  \n",
    "def coef(x, y): \n",
    "    # number of observations/points \n",
    "    n = np.size(x) \n",
    "  \n",
    "    # mean of x and y vector \n",
    "    m_x, m_y = np.mean(x), np.mean(y) \n",
    "  \n",
    "    # calculating cross-deviation and deviation about x \n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "  \n",
    "    # calculating regression coefficients \n",
    "    b_1 = SS_xy / SS_xx \n",
    "    b_0 = m_y - b_1*m_x \n",
    "  \n",
    "    return np.asarray((b_0, b_1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter Tuning: \n",
    "#####  (i) For EnKBF, in order to target a certain MSE value, we need to banlance this two terms above as \n",
    "$$\n",
    "\\mathbb{E}[\\|\\eta_{t}^{N,l}(e)-\\eta_{t}(e)\\|^2]  = \\mathbb{E}[\\|\\eta_{t}^{N,l}(e) - \\eta_{t}^{l}(e)\\|^2] + \\|\\eta_{t}^{l}(e) - \\eta_{t}(e)\\|^2\n",
    "$$\n",
    "\n",
    "From Proposition 2.1 of our paper, we know that EnKBF estimator $\\eta_{t}^{N,l}(e)$ satisfies\n",
    "\n",
    "1. $ \\mathbb{E}[\\|\\eta_{t}^{N,l}(e) - \\eta_{t}^{l}(e)\\|^2] \\approx C_{1}(\\frac{1}{N})$\n",
    "\n",
    "2. $  \\|\\eta_{t}^{l}(e) - \\eta_{t}(e)\\| \\approx C_{2}(\\Delta_{l})$\n",
    "\n",
    "We simulate to obtain this two constants $C_{1}$, $C_{2}$. Values of $\\eta_{t}^{l}(\\varphi)$ can be approximated with EnKBF with huge $N$.\n",
    "\n",
    "##### To target a MSE of $\\mathcal{O}(\\epsilon^2)$, we choose $N = \\epsilon^{-2}C_{1}$, $\\Delta_{l} = \\frac{\\epsilon}{C_{2}}$, the cost required for EnKBF is represented by the number of discretization steps involved\n",
    "$$\n",
    "Cost_{enkbf} = N \\Delta_{l}^{-1} t = tC_{1}C_{2}\\epsilon^{-3} = \\mathcal{O}(\\epsilon^{-3})\n",
    "$$\n",
    "\n",
    "##### (ii) For MLEnKBF,  \n",
    "\n",
    "Remark: For the models used in the simulation, EnKBF with $l \\leq 2$ sometimes output exploding values, for $l >2$ this issue doesn't exist, which suggest that we should consider using for MLEnKBF for $L>3$\n",
    "$$\n",
    "\\eta_{t}^{ML}(e) = \\eta_{t}^{N_{3},3}(e) + \\sum_{l=4}^{L} (\\eta_{t}^{l}-\\eta_{t}^{l-1})^{N_{l}}(e)\n",
    "$$\n",
    "to simply avoid getting exploding values from our specific model settings.\n",
    "\n",
    "The MSE of the MLEnKBF estimator is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\|\\eta_{t}^{ML}(e)-\\eta_{t}(e)\\|^2]  = \\mathbb{E}[\\|\\eta_{t}^{ML}(e) - \\eta_{t}^{L}(e)\\|^2] + \\|\\eta_{t}^{L}(e) - \\eta_{t}(e)\\|^2\n",
    "$$\n",
    "\n",
    "From Theorem 3.1 of our paper, we know that MLEnKBF estimator satisfies\n",
    "\n",
    "1. $\\mathbb{E}[\\|\\eta_{t}^{ML}(e) - \\eta_{t}^{L}(e)\\|^2] \\approx \\frac{C_{1}}{N_{3}} + C_{3} \\sum_{l=4}^{L} \\frac{\\Delta_{l}}{N_{l}} + C_{4}\\sum_{l_{1}=4}^{L}\\sum_{l_{2}=4, l_{2} \\neq l_{1}}^{L}\\frac{\\Delta_{l_{1}}}{N_{l_{1}}}\\frac{\\Delta_{l_{2}}}{N_{l_{2}}} \\approx \\frac{C_{1}}{N_{3}} + C_{3} \\sum_{l=4}^{L} \\frac{\\Delta_{l}}{N_{l}}$\n",
    "\n",
    "We remark here that the third term above is in practice much smaller than the first & second term thus can be ignored\n",
    "\n",
    "In order to tune values of $C_3$, recall that Proposition D.4 gives\n",
    "\n",
    "2. $\\mathbb{E}[\\|(\\eta_{t}^{l}-\\eta_{t}^{l-1})^{N_l}(e) - (\\eta_{t}^{l}-\\eta_{t}^{l-1})(e)\\|^2] \\approx C_{3} \\frac{\\Delta_{l}}{N_{l}}$\n",
    "\n",
    "We can thus simulate CEnKBF to obtain $C_{3}$ for our specific model, too.\n",
    "\n",
    "##### To target a MSE of $\\mathcal{O}(\\epsilon^{2})$ for MLEnKBF , we choose $\\Delta_{L} = \\frac{\\epsilon}{C_{2}}$, $N_{3}=C_{1}\\epsilon^{-2}$, $N_{l} = \\epsilon^{-2}\\Delta_{l}C_{3}(L-3) $ for $l \\in \\{4,5,...,L\\}$, the cost required for MLEnKBF is represented by the number of discretization steps involved\n",
    "$$\n",
    "Cost_{mlenkbf} = t\\sum_{l=3}^{L}N_{l}\\Delta_{l}^{-1} = tC_{1}\\Delta_{3}^{-1}\\epsilon^{-2} + \\sum_{l=4}^{L}C_{3}(L-3)\\epsilon^{-2} = \\mathcal{O}(\\epsilon^{-2}\\log(\\epsilon)^2)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### For Determinisitc D-EnKBF/MLD-EnKBF, similar tuning is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In order to construct (D)MLEnKBF, we will need values of c1,c2,c3 to be fitted out first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Numcal(C,L):\n",
    "    Num=np.zeros(L+1)\n",
    "    epsilon = c2 * 2**(-L)\n",
    "    Num[3] = int(c1 * epsilon**(-2))\n",
    "    for li in range(4,L+1):\n",
    "        Num[li]=int(epsilon**(-2)*c3*(L-3)*2**(-l))\n",
    "    return(Num*C)\n",
    "\n",
    "def MLEnKBF(C,T,L,lmax,z,collection_input):\n",
    "    Num=Numcal(C,L)\n",
    "    telescopic_summand=np.zeros(L-3+1)\n",
    "    [m,c]=EnKBF(T,3,lmax,z,int(Num[3]),collection_input)\n",
    "    telescopic_summand[0]=m[-1,0]\n",
    "    for l in range(4,L+1):\n",
    "        [m1,m2]=CEnKBF(T,l,lmax,z,int(Num[l]),collection_input)\n",
    "        telescopic_summand[l-3]=m1[-1,0]-m2[-1,0]\n",
    "    est=np.sum(telescopic_summand)\n",
    "    return(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_enkbf(T,L):\n",
    "    epsilon = c2 * 2**(-L)\n",
    "    cost=int(T*c1*c2*epsilon**(-3))\n",
    "    return(cost)\n",
    "\n",
    "def cost_mlenkbf(C,T,L):\n",
    "    num=Numcal(C,L)\n",
    "    kk=0\n",
    "    for l in range(3,L+1):\n",
    "        kk=kk+num[l]*(2**(l))\n",
    "    return(T*kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Numcal_D(C,L):\n",
    "    Num=np.zeros(L+1)\n",
    "    epsilon = c2 * 2**(-L)\n",
    "    Num[3] = int(c1 * epsilon**(-2))\n",
    "    for li in range(4,L+1):\n",
    "        Num[li]=int(epsilon**(-2)*c3*(L-3)*2**(-l))\n",
    "    return(Num*C)\n",
    "\n",
    "def MLDEnKBF(C,T,L,lmax,z,collection_input):\n",
    "    Num=Numcal_D(C,L)\n",
    "    telescopic_summand=np.zeros(L-3+1)\n",
    "    [m,c]=DEnKBF(T,3,lmax,z,int(Num[3]),collection_input)\n",
    "    telescopic_summand[0]=m[-1,0]\n",
    "    for l in range(4,L+1):\n",
    "        [m1,m2]=DCEnKBF(T,l,lmax,z,int(Num[l]),collection_input)\n",
    "        telescopic_summand[l-3]=m1[-1,0]-m2[-1,0]\n",
    "    est=np.sum(telescopic_summand)\n",
    "    return(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_denkbf(T,L):\n",
    "    epsilon = c2 * 2**(-L)\n",
    "    cost=int(T*c1*c2*epsilon**(-3))\n",
    "    return(cost)\n",
    "\n",
    "def cost_mldenkbf(C,T,L):\n",
    "    num=Numcal_D(C,L)\n",
    "    kk=0\n",
    "    for l in range(3,L+1):\n",
    "        kk=kk+num[l]*(2**(l))\n",
    "    return(T*kk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. $\\mathbb{E}[\\|(\\eta_{t}^{l,N}(\\varphi)-\\eta_{t}^{l}(\\varphi))\\|^2] \\approx C_{1}(\\frac{1}{N})$\n",
    "##### 2. $\\|\\eta_{t}^{l}(e) - \\eta_{t}(e)\\| \\approx C_{2}(\\Delta_{l})$\n",
    "##### 3. $\\mathbb{E}[\\|(\\eta_{t}^{l}-\\eta_{t}^{l-1})^{N_l}(e) - (\\eta_{t}^{l}-\\eta_{t}^{l-1})(e)\\|^2] \\approx C_{3} \\frac{\\Delta_{l}}{N_{l}}$\n",
    "##### 4. Simulation to get MSE of MLEnKBF & EnKBF for $l \\in \\{4,5,6,7,8,9\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_c1(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    dim = 10\n",
    "    l=4\n",
    "    lmax=9\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    num_seq = np.zeros(6)\n",
    "    for i in range(6):\n",
    "        num_seq[i] = 10*2**i\n",
    "        \n",
    "    Rep=100\n",
    "    evar=np.zeros(6)\n",
    "    for numi in range(6):\n",
    "        est=np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                mean_mat = EnKBF(T,l,lmax,z,int(num_seq[numi]),collection_input)[0]\n",
    "                est[rep] = np.mean(mean_mat[-1,:])\n",
    "        evar[numi] = np.var(est)\n",
    "    \n",
    "    x = np.log10(num_seq)\n",
    "    y = np.log10(evar)\n",
    "    b = coef(x,y)\n",
    "    return 10**(b[0])\n",
    "\n",
    "def fit_c2(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    lmax = 12\n",
    "    dim = 10\n",
    "    T=100\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    N=1000\n",
    "    Rep=100\n",
    "    tv_rep = np.zeros((Rep,dim))\n",
    "    for rep in range(Rep):\n",
    "        tv_rep[rep] = EnKBF(T,lmax,lmax,z,N,collection_input)[0][-1,:][:,0]\n",
    "    tv_approx = np.mean(tv_rep,axis=0)\n",
    "    \n",
    "    bias_level=np.zeros(10)\n",
    "    delt_level=np.zeros(10)\n",
    "    for l in range(4,10):\n",
    "        est=np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                mean_mat = EnKBF(T,l,lmax,z,N,collection_input)[0][-1,:]\n",
    "                est[rep] = np.sqrt(np.sum((mean_mat - tv_approx)**2))\n",
    "        bias_level[l] = np.mean(est)\n",
    "        delt_level[l] = 2**(-l)\n",
    "    \n",
    "    x = np.log10(delt_level[4:10])\n",
    "    y = np.log10(bias_level[4:10])\n",
    "    b = coef(x,y)\n",
    "    return 10**(b[0])\n",
    "\n",
    "def fit_c3(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    dim = 10\n",
    "    lmax=9\n",
    "    N = 50\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    delta_seq = np.zeros(10)\n",
    "    for l in range(10):\n",
    "        delta_seq[l] = 2**(-l)\n",
    "    \n",
    "    Rep=100\n",
    "    evar=np.zeros(10)\n",
    "    for l in range(4,8):\n",
    "        est = np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                [m,m1]=CEnKBF(T,l,lmax,z,N,collection_input)\n",
    "                est[rep] = np.mean(m[-1,:]) - np.mean(m1[-1,:])\n",
    "        evar[l] = np.var(est)\n",
    "        \n",
    "    x = np.log10(delta_seq[4:8])\n",
    "    y = np.log10(evar[4:8])\n",
    "    b=coef(x,y)\n",
    "    return N*10**(b[0])\n",
    "\n",
    "def simulate_mse(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    lmax = 12\n",
    "    dim = 10\n",
    "    T=100\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    N=1000\n",
    "    \n",
    "    tv_rep = np.zeros((Rep,dim))\n",
    "    for rep in range(Rep):\n",
    "        tv_rep[rep] = EnKBF(T,lmax,lmax,z,N,collection_input)[0][-1,:][:,0]\n",
    "    tv_approx = np.mean(tv_rep,axis=0)\n",
    "    mse_level=np.zeros(10)\n",
    "    delt_level=np.zeros(10)\n",
    "    C=2.1\n",
    "    Rep=100\n",
    "    for l in range(4,10):\n",
    "        est_ml=np.zeros(Rep)\n",
    "        est_en=np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                mean_en = EnKBF(T,l,lmax,z,N,collection_input)[0][-1,:]\n",
    "                mean_ml = MLEnKBF(C,T,L,lmax,z,collection_input)\n",
    "                est_ml[rep] = np.sum((mean_ml - tv_approx)**2)\n",
    "                est_en[rep] = np.sum((mean_en - tv_approx)**2)\n",
    "        mse_ml[l] = np.mean(est_ml)\n",
    "        mse_en[l] = np.mean(est_en)\n",
    "    \n",
    "    return mse_ml[4:10],mse_en[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. $\\mathbb{E}[\\|(\\eta_{t}^{l,N}(\\varphi)-\\eta_{t}^{l}(\\varphi))\\|^2] \\approx C_{1}(\\frac{1}{N})$\n",
    "##### 2. $\\|\\eta_{t}^{l}(e) - \\eta_{t}(e)\\| \\approx C_{2}(\\Delta_{l})$\n",
    "##### 3. $\\mathbb{E}[\\|(\\eta_{t}^{l}-\\eta_{t}^{l-1})^{N_l}(e) - (\\eta_{t}^{l}-\\eta_{t}^{l-1})(e)\\|^2] \\approx C_{3} \\frac{\\Delta_{l}}{N_{l}}$\n",
    "##### 4. Simulation to get MSE of D-MLEnKBF & D-EnKBF for $l \\in \\{4,5,6,7,8,9\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_c1(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    dim = 10\n",
    "    l=4\n",
    "    lmax=9\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    num_seq = np.zeros(6)\n",
    "    for i in range(6):\n",
    "        num_seq[i] = 10*2**i\n",
    "        \n",
    "    Rep=100\n",
    "    evar=np.zeros(6)\n",
    "    for numi in range(6):\n",
    "        est=np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                mean_mat = DEnKBF(T,l,lmax,z,int(num_seq[numi]),collection_input)[0]\n",
    "                est[rep] = np.mean(mean_mat[-1,:])\n",
    "        evar[numi] = np.var(est)\n",
    "    \n",
    "    x = np.log10(num_seq)\n",
    "    y = np.log10(evar)\n",
    "    b = coef(x,y)\n",
    "    return 10**(b[0])\n",
    "\n",
    "def fit_c2(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    lmax = 12\n",
    "    dim = 10\n",
    "    T=100\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    N=1000\n",
    "    Rep=100\n",
    "    tv_rep = np.zeros((Rep,dim))\n",
    "    for rep in range(Rep):\n",
    "        tv_rep[rep] = EnKBF(T,lmax,lmax,z,N,collection_input)[0][-1,:][:,0]\n",
    "    tv_approx = np.mean(tv_rep,axis=0)\n",
    "    \n",
    "    bias_level=np.zeros(10)\n",
    "    delt_level=np.zeros(10)\n",
    "    for l in range(4,10):\n",
    "        est=np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                mean_mat = DEnKBF(T,l,lmax,z,N,collection_input)[0][-1,:]\n",
    "                est[rep] = np.sqrt(np.sum((mean_mat - tv_approx)**2))\n",
    "        bias_level[l] = np.mean(est)\n",
    "        delt_level[l] = 2**(-l)\n",
    "    \n",
    "    x = np.log10(delt_level[4:10])\n",
    "    y = np.log10(bias_level[4:10])\n",
    "    b = coef(x,y)\n",
    "    return 10**(b[0])\n",
    "\n",
    "def fit_c3(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    dim = 10\n",
    "    lmax=9\n",
    "    N = 50\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    delta_seq = np.zeros(10)\n",
    "    for l in range(10):\n",
    "        delta_seq[l] = 2**(-l)\n",
    "    \n",
    "    Rep=100\n",
    "    evar=np.zeros(10)\n",
    "    for l in range(4,8):\n",
    "        est = np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                [m,m1]=DCEnKBF(T,l,lmax,z,N,collection_input)\n",
    "                est[rep] = np.mean(m[-1,:]) - np.mean(m1[-1,:])\n",
    "        evar[l] = np.var(est)\n",
    "        \n",
    "    x = np.log10(delta_seq[4:8])\n",
    "    y = np.log10(evar[4:8])\n",
    "    b=coef(x,y)\n",
    "    return N*10**(b[0])\n",
    "\n",
    "def simulate_mse(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    lmax = 12\n",
    "    dim = 10\n",
    "    T=100\n",
    "    collection_input = gen_model(dim)\n",
    "    z = gen_data(T,lmax,collection_input)[0]\n",
    "    N=1000\n",
    "    \n",
    "    tv_rep = np.zeros((Rep,dim))\n",
    "    for rep in range(Rep):\n",
    "        tv_rep[rep] = EnKBF(T,lmax,lmax,z,N,collection_input)[0][-1,:][:,0]\n",
    "    tv_approx = np.mean(tv_rep,axis=0)\n",
    "    mse_level=np.zeros(10)\n",
    "    delt_level=np.zeros(10)\n",
    "    C=2.1\n",
    "    Rep=100\n",
    "    for l in range(4,10):\n",
    "        est_ml=np.zeros(Rep)\n",
    "        est_en=np.zeros(Rep)\n",
    "        for rep in range(Rep):\n",
    "            with np.errstate(divide='ignore'):\n",
    "                mean_en = DEnKBF(T,l,lmax,z,N,collection_input)[0][-1,:]\n",
    "                mean_ml = MLDEnKBF(C,T,L,lmax,z,collection_input)\n",
    "                est_ml[rep] = np.sum((mean_ml - tv_approx)**2)\n",
    "                est_en[rep] = np.sum((mean_en - tv_approx)**2)\n",
    "        mse_ml[l] = np.mean(est_ml)\n",
    "        mse_en[l] = np.mean(est_en)\n",
    "    \n",
    "    return mse_ml,mse_en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
